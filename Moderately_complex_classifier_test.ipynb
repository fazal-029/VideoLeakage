{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3a2d424-ed41-404b-9b73-ed24ecabe926",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f96cab7-d5d3-4f4b-9168-1b020a7139da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Running on cpu\n"
     ]
    }
   ],
   "source": [
    "tp = transforms.Compose([\n",
    "    transforms.Resize(32),\n",
    "    transforms.CenterCrop(32),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "tt = transforms.ToPILImage()\n",
    "dst = datasets.CIFAR100(\"~/.torch\", download=True, transform=tp)\n",
    "\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "print(\"Running on %s\" % device)\n",
    "\n",
    "def label_to_onehot(target, num_classes=100):\n",
    "    target = torch.unsqueeze(target, 1)\n",
    "    onehot_target = torch.zeros(target.size(0), num_classes, device=target.device)\n",
    "    onehot_target.scatter_(1, target, 1)\n",
    "    return onehot_target\n",
    "\n",
    "def cross_entropy_for_onehot(pred, target):\n",
    "    return torch.mean(torch.sum(- target * F.log_softmax(pred, dim=-1), 1))\n",
    "\n",
    "def weights_init(m):\n",
    "    try:\n",
    "        if hasattr(m, \"weight\") and m.weight is not None:\n",
    "            m.weight.data.uniform_(-0.5, 0.5)\n",
    "    except Exception as e:\n",
    "        print(f'Warning: failed in weights_init for {m._get_name()}.weight. Details: {e}')\n",
    "    try:\n",
    "        if hasattr(m, \"bias\") and m.bias is not None:  # Added 'and m.bias is not None'\n",
    "            m.bias.data.uniform_(-0.5, 0.5)\n",
    "    except Exception as e:\n",
    "        print(f'Warning: failed in weights_init for {m._get_name()}.bias. Details: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a623c6eb-44f2-4066-af77-8a890c964a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ResNet-20 (for CIFAR-10)\n",
    "class ResNet20(nn.Module):\n",
    "    def __init__(self, num_classes=100):\n",
    "        super(ResNet20, self).__init__()\n",
    "        self.in_channels = 16\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.layer1 = self._make_layer(16, 3, stride=1)\n",
    "        self.layer2 = self._make_layer(32, 3, stride=2)\n",
    "        self.layer3 = self._make_layer(64, 3, stride=2)\n",
    "        self.linear = nn.Linear(64, num_classes)\n",
    "\n",
    "    def _make_layer(self, out_channels, num_blocks, stride):\n",
    "        layers = []\n",
    "        layers.append(ResBlock(self.in_channels, out_channels, stride))\n",
    "        self.in_channels = out_channels\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(ResBlock(out_channels, out_channels, stride=1))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = F.avg_pool2d(out, out.size()[3])\n",
    "        out = out.view(out.size(0), -1)\n",
    "        return self.linear(out)\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        return F.relu(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97af2260-f677-4f8f-9b55-0c5d8013bc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_shared = ResNet20().to(device)\n",
    "model_shared.load_state_dict(torch.load(\"resnet20_cifar10.pth\"))\n",
    "\n",
    "# 2. Freeze all weights\n",
    "for param in model_shared.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 3. Remove the final classification layer\n",
    "# (Replace it with Identity() to keep the feature vector)\n",
    "model_shared.linear = nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f9e2fbc-3349-4d55-b238-b6d2dd6e86f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simplest classifier\n",
    "class Classifier1(nn.Module):\n",
    "    def __init__(self, input_dim=64, num_classes=100):\n",
    "        super(Classifier1, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 32)\n",
    "        self.fc2 = nn.Linear(32, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "# A bit more complex than classifier 1.\n",
    "class Classifier2(nn.Module):\n",
    "    def __init__(self, input_dim=64, num_classes=100, dropout_prob=0.3):\n",
    "        super(Classifier2, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)  # Slightly wider first layer\n",
    "        self.fc2 = nn.Linear(128, 64)         # Intermediate layer\n",
    "        self.fc3 = nn.Linear(64, num_classes) # Output layer\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc3(x)\n",
    "\n",
    "# A bit more complex than classifier 2.\n",
    "class Classifier3(nn.Module):\n",
    "    def __init__(self, input_dim=64, num_classes=100, dropout_prob=0.2):\n",
    "        super(Classifier3, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.norm1 = nn.LayerNorm(64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.norm2 = nn.LayerNorm(32)\n",
    "        self.fc3 = nn.Linear(32, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.norm1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.norm2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc3(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e701ca9-e0be-4e71-89f7-3427d1b4cb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Classifier2().to(device)\n",
    "classifier.apply(weights_init)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=0.001)\n",
    "def cross_entropy_for_onehot(pred, target):\n",
    "    return torch.mean(torch.sum(- target * F.log_softmax(pred, dim=-1), 1))\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b493b44e-9e92-47d7-8eb5-0b08bd4cb4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GT label is 84. \n",
      "Onehot label is 84.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "######### honest partipant #########\n",
    "img_index = 25\n",
    "gt_data = dst[img_index][0].to(device)\n",
    "gt_data = gt_data.view(1, *gt_data.size())\n",
    "gt_label = torch.Tensor([dst[img_index][1]]).long().to(device)\n",
    "gt_label = gt_label.view(1, )\n",
    "gt_onehot_label = label_to_onehot(gt_label, num_classes=100)\n",
    "\n",
    "plt.imshow(tt(gt_data[0].cpu()))\n",
    "plt.title(\"Ground truth image\")\n",
    "print(\"GT label is %d.\" % gt_label.item(), \"\\nOnehot label is %d.\" % torch.argmax(gt_onehot_label, dim=-1).item())\n",
    "# plt.savefig('image1.png')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8780893d-0fd2-4165-bb28-1208bcd280f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute original gradient \n",
    "ground_truth_feature_matrix =  model_shared(gt_data) # torch.randn((1, 64)).to(device).requires_grad_(True) \n",
    "out = classifier(ground_truth_feature_matrix)\n",
    "y = criterion(out, gt_onehot_label)\n",
    "dy_dx = torch.autograd.grad(y, classifier.parameters())\n",
    "# share the gradients with other clients\n",
    "original_dy_dx = list((_.detach().clone() for _ in dy_dx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181e086c-48c7-4431-bc2b-1ae2014e9b4c",
   "metadata": {},
   "source": [
    "# DLG starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "318f9fe1-e0fb-43ca-9ef0-adfe42d58248",
   "metadata": {},
   "outputs": [],
   "source": [
    "after_50_iteration = 999999\n",
    "best_state = {\n",
    "    'loss': 99999,\n",
    "    'reconstructed_matrix': None\n",
    "}\n",
    "\n",
    "for i in range(50):\n",
    "    # generate dummy data and label\n",
    "    dummy_data = torch.randn(ground_truth_feature_matrix.size()).to(device).requires_grad_(True)\n",
    "    dummy_label = torch.randn(gt_onehot_label.size()).to(device).requires_grad_(True)\n",
    "    optimizer = torch.optim.LBFGS([dummy_data, dummy_label] )\n",
    "    current_loss = None\n",
    "    \n",
    "    history = []\n",
    "    for iters in range(500):\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            pred = classifier(dummy_data) \n",
    "            dummy_onehot_label = F.softmax(dummy_label, dim=-1)\n",
    "            dummy_loss = criterion(pred, dummy_onehot_label)\n",
    "            dummy_dy_dx = torch.autograd.grad(dummy_loss, classifier.parameters(), create_graph=True)\n",
    "            \n",
    "            grad_diff = 0\n",
    "            grad_count = 0\n",
    "            for gx, gy in zip(dummy_dy_dx, original_dy_dx): # TODO: fix the variablas here\n",
    "                grad_diff += ((gx - gy) ** 2).sum()\n",
    "                grad_count += gx.nelement()\n",
    "            # grad_diff = grad_diff / grad_count * 1000\n",
    "            grad_diff.backward()\n",
    "            \n",
    "            return grad_diff\n",
    "        \n",
    "        optimizer.step(closure)\n",
    "        if iters % 20 == 0: \n",
    "            current_loss = closure()\n",
    "            # print(iters, \"%.4f\" % current_loss.item())\n",
    "        history.append(dummy_data[0].cpu())\n",
    "        \n",
    "    if current_loss < after_50_iteration:\n",
    "        after_50_iteration = current_loss\n",
    "        best_state['loss'] = current_loss\n",
    "        best_state['reconstructed_matrix'] = dummy_data\n",
    "    if current_loss < 0.00001:\n",
    "        best_state['loss'] = current_loss\n",
    "        best_state['reconstructed_matrix'] = dummy_data\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f271b4b2-f815-4482-af52-8244d60be8f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best results from 50 iterations\n",
      "best loss: 31268.9140625\n",
      "reconstructed data: tensor([[  -3.8668, -125.8545, -180.9626,  -74.6563,   58.8770,  237.1508,\n",
      "           58.7959, -138.1024, -105.7960,   13.3949, -148.8306,   72.5349,\n",
      "         -232.7437,  210.4364,  145.7260,  159.9144,  -49.7592, -172.6244,\n",
      "          -29.3233,  -72.5181, -119.6697,  142.1348,    5.9675,  209.8351,\n",
      "         -116.9721,    0.4430,   26.0194,  -77.2023,  115.3932,  216.8689,\n",
      "         -166.5563,   88.3353,  -31.2429,  -59.3146,   28.0099, -195.9978,\n",
      "         -109.2310,   99.0364,  -64.0598,  -30.6986,  -41.9937,  -56.8957,\n",
      "         -125.7026,    5.7151, -203.9180, -150.7401,   96.6960, -201.1729,\n",
      "          150.0789,   79.8496, -186.3351,  -65.2328,  -22.1324,  -15.5192,\n",
      "          202.8631,   90.1464,   43.8237,  -55.9034,  -50.2190,   73.6373,\n",
      "          -63.9623,   58.8460,  -56.0812, -199.5088]], requires_grad=True), ground truth tensor([[1.6594, 1.6851, 1.5739, 1.5922, 1.6475, 1.9456, 1.5760, 1.5572, 1.7283,\n",
      "         1.3682, 1.4964, 1.3229, 1.4383, 1.6828, 1.4704, 1.6110, 1.5296, 1.0040,\n",
      "         1.6437, 1.8951, 1.6850, 1.4664, 1.4830, 1.4127, 1.5113, 1.3804, 1.6133,\n",
      "         1.6509, 1.5186, 1.5320, 1.6058, 1.5622, 1.6572, 1.4124, 1.1097, 1.5419,\n",
      "         1.5737, 1.3950, 1.5675, 1.5751, 1.6170, 1.4995, 1.5029, 1.7241, 1.7387,\n",
      "         1.5396, 1.1590, 1.5140, 1.0932, 1.5765, 1.3976, 1.7565, 1.4108, 1.3665,\n",
      "         1.1500, 1.4556, 1.5779, 1.3575, 1.8859, 1.4348, 1.6789, 1.3101, 1.3429,\n",
      "         1.8155]])\n"
     ]
    }
   ],
   "source": [
    "print('best results from 50 iterations')\n",
    "print(f\"best loss: {best_state['loss']}\")\n",
    "print(f\"reconstructed data: {best_state['reconstructed_matrix']}, ground truth {ground_truth_feature_matrix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7109fb4a-ccd6-45d7-90cc-9954f85eef4c",
   "metadata": {},
   "source": [
    "# iDLG starts here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74e05519-a1aa-4aa6-aafe-f1bd1aa67ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_idlg = classifier(ground_truth_feature_matrix)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "y = criterion(out_idlg, gt_label)\n",
    "dy_dx = torch.autograd.grad(y, classifier.parameters())\n",
    "# share the gradients with other clients\n",
    "original_dy_dx = list((_.detach().clone() for _ in dy_dx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d50609b0-b960-4ccc-94db-3f8cb068a032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([84]) tensor([84])\n"
     ]
    }
   ],
   "source": [
    "label_pred =  torch.argmin(torch.sum(original_dy_dx[-2], dim=-1), dim=-1).detach().reshape((1,)).requires_grad_(False)\n",
    "print(gt_label, label_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10e0f087-3a40-44b8-8bc5-434f8f1d676a",
   "metadata": {},
   "outputs": [],
   "source": [
    "after_50_iteration = 999999\n",
    "best_state = {\n",
    "    'loss': 99999,\n",
    "    'reconstructed_matrix': None\n",
    "}\n",
    "\n",
    "for i in range(50):\n",
    "    history = []\n",
    "    dummy_data = torch.randn(ground_truth_feature_matrix.size()).to(device).requires_grad_(True)\n",
    "    optimizer = torch.optim.Adam([dummy_data], lr=0.01)\n",
    "    current_loss = None\n",
    "    \n",
    "    for iters in range(2000):\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            pred = classifier(dummy_data) \n",
    "            dummy_loss = criterion(pred, label_pred)\n",
    "            dummy_dy_dx = torch.autograd.grad(dummy_loss, classifier.parameters(), create_graph=True)\n",
    "            \n",
    "            grad_diff = 0\n",
    "            grad_count = 0\n",
    "            for gx, gy in zip(dummy_dy_dx, original_dy_dx): # TODO: fix the variablas here\n",
    "                grad_diff += ((gx - gy) ** 2).sum()\n",
    "                grad_count += gx.nelement()\n",
    "            # grad_diff = grad_diff / grad_count * 1000\n",
    "            grad_diff.backward()\n",
    "            \n",
    "            return grad_diff\n",
    "        \n",
    "        optimizer.step(closure)\n",
    "        if iters % 100 == 0: \n",
    "            current_loss = closure()\n",
    "            # print(iters, \"%.4f\" % current_loss.item())\n",
    "        history.append(dummy_data[0].cpu())\n",
    "    if current_loss < after_50_iteration:\n",
    "        after_50_iteration = current_loss\n",
    "        best_state['loss'] = current_loss\n",
    "        best_state['reconstructed_matrix'] = dummy_data\n",
    "    if current_loss < 0.00001:\n",
    "        best_state['loss'] = current_loss\n",
    "        best_state['reconstructed_matrix'] = dummy_data\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98df85b4-89a7-4da2-9815-763b70dca879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best results from 50 iterations\n",
      "best loss: 19458.314453125\n",
      "reconstructed data: tensor([[ 0.6114,  0.4547,  0.8057,  0.3445,  0.2909,  0.9583,  0.4908, -0.2084,\n",
      "          0.8447,  0.7503,  0.4710,  0.1208,  0.0147,  0.3091,  0.4054,  0.2314,\n",
      "          0.2922, -0.1382, -0.1419,  0.2927,  0.3632,  0.2056,  0.4264,  0.1453,\n",
      "          0.2593,  0.8768,  0.1191,  0.4203,  0.4049,  0.8425, -0.1377, -0.1704,\n",
      "          0.5864,  0.4980,  0.7709,  0.5770,  0.1834,  0.5530,  0.8915,  1.2203,\n",
      "          0.5503,  1.0114,  0.4817,  1.2081,  0.5326, -0.0424, -0.4816,  0.2991,\n",
      "          0.1812,  0.6026,  0.2039,  1.0254,  0.3908,  0.2498,  0.3799,  0.0597,\n",
      "          1.0006, -0.2122, -0.1720, -0.3960,  0.9948,  0.3160,  0.0664,  0.4414]],\n",
      "       requires_grad=True), ground truth tensor([[1.6594, 1.6851, 1.5739, 1.5922, 1.6475, 1.9456, 1.5760, 1.5572, 1.7283,\n",
      "         1.3682, 1.4964, 1.3229, 1.4383, 1.6828, 1.4704, 1.6110, 1.5296, 1.0040,\n",
      "         1.6437, 1.8951, 1.6850, 1.4664, 1.4830, 1.4127, 1.5113, 1.3804, 1.6133,\n",
      "         1.6509, 1.5186, 1.5320, 1.6058, 1.5622, 1.6572, 1.4124, 1.1097, 1.5419,\n",
      "         1.5737, 1.3950, 1.5675, 1.5751, 1.6170, 1.4995, 1.5029, 1.7241, 1.7387,\n",
      "         1.5396, 1.1590, 1.5140, 1.0932, 1.5765, 1.3976, 1.7565, 1.4108, 1.3665,\n",
      "         1.1500, 1.4556, 1.5779, 1.3575, 1.8859, 1.4348, 1.6789, 1.3101, 1.3429,\n",
      "         1.8155]])\n"
     ]
    }
   ],
   "source": [
    "print('best results from 50 iterations')\n",
    "print(f\"best loss: {best_state['loss']}\")\n",
    "print(f\"reconstructed data: {best_state['reconstructed_matrix']}, ground truth {ground_truth_feature_matrix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c6b3ff-a456-4bd6-8b47-421610d142f6",
   "metadata": {},
   "source": [
    "# R-GAP starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08478a8e-25be-4cdb-a837-d42f3688d840",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier2_R_GAP(nn.Module):\n",
    "    def __init__(self, input_dim=64, num_classes=1, dropout_prob=0.3): # num_classes often 1 for these attacks\n",
    "        super(Classifier2_R_GAP, self).__init__()\n",
    "        \n",
    "        act = nn.LeakyReLU(negative_slope=0.2)\n",
    "\n",
    "        self.body = nn.ModuleList([\n",
    "            # Block 1, corresponding to original fc1\n",
    "            nn.Sequential(OrderedDict([\n",
    "                ('layer', nn.Linear(input_dim, 128, bias=False)),\n",
    "                ('act', act)\n",
    "            ])),\n",
    "            # Block 2, corresponding to original fc2\n",
    "            nn.Sequential(OrderedDict([\n",
    "                ('layer', nn.Linear(128, 64, bias=False)),\n",
    "                ('act', act)\n",
    "            ])),\n",
    "            # Block 3, corresponding to original fc3 (output layer)\n",
    "            nn.Sequential(OrderedDict([\n",
    "                ('layer', nn.Linear(64, num_classes, bias=False)),\n",
    "                ('act', nn.Identity()) # No activation after the final layer\n",
    "            ]))\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_shape = []\n",
    "\n",
    "        for i, module_seq in enumerate(self.body):\n",
    "            if isinstance(module_seq.layer, nn.Linear) and x.dim() > 2:\n",
    "                if i == 0 or not isinstance(self.body[i-1].layer, nn.Linear):\n",
    "                    x = x.flatten(1)\n",
    "            \n",
    "            x_shape.append(x.shape)\n",
    "            x = module_seq(x)\n",
    "            \n",
    "        return x, x_shape\n",
    "classifier = Classifier2_R_GAP().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6fff377e-251b-4c8f-9e3e-d657ae74045d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_loss(y, pred, setup):\n",
    "    y = torch.as_tensor(y, **setup)\n",
    "    pred = torch.as_tensor(pred, **setup)\n",
    "\n",
    "    if pred.shape != y.shape:\n",
    "        y = y.view_as(pred)\n",
    "    eps = 1e-7\n",
    "    pred_clamped = torch.clamp(pred, eps, 1.0 - eps)\n",
    "    loss = -(y * torch.log(pred_clamped) + (1 - y) * torch.log(1 - pred_clamped))\n",
    "    return torch.mean(loss)\n",
    "\n",
    "pred, x_shape = classifier(ground_truth_feature_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d829ebe1-8125-4c55-bae8-c4646c89da43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_udldu(udldu):\n",
    "    '''derive u from udldu using gradient descend based method'''\n",
    "    lr = 0.01\n",
    "    u = torch.tensor(0).to(**setup).requires_grad_(True)\n",
    "    udldu = torch.tensor(udldu).to(**setup)\n",
    "    optimizer = torch.optim.Adam([u], lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    for i in range(30000):\n",
    "        optimizer.zero_grad()\n",
    "        udldu_ = -u / (1 + torch.exp(u))\n",
    "        l = loss_fn(udldu_, udldu)\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "    udldu_ = -u / (1 + torch.exp(u))\n",
    "    print(f\"The error term of inversing udldu: {udldu.item()-udldu_.item():.1e}\")\n",
    "    return u.detach().numpy()\n",
    "\n",
    "def derive_leakyrelu(x, slope):\n",
    "    return np.array([slope if a < 0 else 1 for a in x]).reshape(1, -1).astype('float32')\n",
    "\n",
    "def derive_identity(x):\n",
    "    return np.ones(x.shape).reshape(1, -1).astype('float32')\n",
    "    \n",
    "def inverse_leakyrelu(x, slope):\n",
    "    return np.array([a / slope if a < 0 else a for a in x]).astype('float32')\n",
    "\n",
    "def inverse_identity(x):\n",
    "    return x\n",
    "\n",
    "def peeling(in_shape, padding):\n",
    "    if padding == 0:\n",
    "        return np.ones(shape=in_shape, dtype=bool).squeeze()\n",
    "    h, w = np.array(in_shape[-2:]) + 2*padding\n",
    "    toremain = np.ones(h*w*in_shape[1], dtype=np.bool)\n",
    "    if padding:\n",
    "        for c in range(in_shape[1]):\n",
    "            for row in range(h):\n",
    "                for col in range(w):\n",
    "                    if col < padding or w-col <= padding or row < padding or h-row <= padding:\n",
    "                        i = c*h*w + row*w + col\n",
    "                        assert toremain[i]\n",
    "                        toremain[i] = False\n",
    "    return toremain\n",
    "\n",
    "def r_gap(out, k, g, x_shape, weight, module):\n",
    "    # obtain information of convolution kernel\n",
    "    if isinstance(module.layer, nn.Conv2d):\n",
    "        padding = module.layer.padding[0]\n",
    "        stride = module.layer.stride[0]\n",
    "    else:\n",
    "        padding = 0\n",
    "        stride = 1\n",
    "\n",
    "    x, weight = cnn_reconstruction(in_shape=x_shape, k=k, g=g, kernel=weight, out=out, stride=stride, padding=padding)\n",
    "    return x, weight\n",
    "\n",
    "def cnn_reconstruction(in_shape, k, g, out, kernel, stride, padding):\n",
    "    coors, x_len, y_len = generate_coordinates(x_shape=in_shape, kernel=kernel, stride=stride, padding=padding)\n",
    "    K = aggregate_g(k=k, x_len=x_len, coors=coors)\n",
    "    W = circulant_w(x_len=x_len, kernel=kernel, coors=coors, y_len=y_len)\n",
    "    P = padding_constraints(in_shape=in_shape, padding=padding)\n",
    "    p = np.zeros(shape=P.shape[0], dtype=np.float32)\n",
    "    if np.any(P):\n",
    "        a = np.concatenate((K, W, P), axis=0)\n",
    "        b = np.concatenate((g.reshape(-1), out, p), axis=0)\n",
    "    else:\n",
    "        a = np.concatenate((K, W), axis=0)\n",
    "        b = np.concatenate((g.reshape(-1), out), axis=0)\n",
    "    result = np.linalg.lstsq(a, b, rcond=None)\n",
    "    print(f'lstsq residual: {result[1]}, rank: {result[2]} -> {W.shape[-1]}, '\n",
    "          f'max/min singular value: {result[3].max():.2e}/{result[3].min():.2e}')\n",
    "    x = result[0]\n",
    "    return x[peeling(in_shape=in_shape, padding=padding)], W\n",
    "\n",
    "def fcn_reconstruction(k, gradient):\n",
    "    x = [g / c for g, c in zip(gradient, k) if c != 0]\n",
    "    x = np.mean(x, 0)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ad7783e-46da-4780-b903-bb7f5f117c0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1]), torch.Size([1, 1]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setup = {'device': 'cpu', 'dtype': torch.float32}\n",
    "# reversed label to make sure mu is unique, just for better demonstration\n",
    "y = pred.clone().detach().to(device) # torch.tensor([0 if p > 0 else 1 for p in pred]).to(**setup)\n",
    "pred.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52fc2520-2108-4f31-a475-061177f948c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: [[-0.16705966]], y: tensor([[-0.1671]])\n"
     ]
    }
   ],
   "source": [
    "print(f'pred: {pred.detach().numpy()}, y: {y}')\n",
    "pred_loss = logistic_loss(pred, y, setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "baf0a7cc-2b41-41fa-9a32-0252d71cebc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dy_dx = torch.autograd.grad(pred_loss, list(classifier.parameters()))\n",
    "original_dy_dx = [g.detach().clone() for g in dy_dx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c68f0665-12a4-42cd-8b38-1211b161017f",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dy_dx.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "98a4bf2c-9e4b-499d-99c5-c9e4e3af9894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perform R-GAP\n",
      "printing udldu -2.692683219909668\n",
      "The error term of inversing udldu: -2.4e+00\n",
      "printing u1.2784645557403564\n"
     ]
    }
   ],
   "source": [
    "modules = classifier.body[-1::-1]\n",
    "x_shape.reverse()\n",
    "k = None\n",
    "last_weight = []\n",
    "y = torch.tensor([0 if p > 0 else 1 for p in gt_label]).to(**setup)\n",
    "\n",
    "print('perform R-GAP')\n",
    "for i in range(len(modules)):\n",
    "    g = original_dy_dx[i].numpy()\n",
    "    w = list(modules[i].layer.parameters())[0].detach().cpu().numpy()\n",
    "    \n",
    "    if k is None:\n",
    "        udldu = np.dot(g.reshape(-1), w.reshape(-1))\n",
    "        print(f'printing udldu {udldu}')\n",
    "        u = inverse_udldu(udldu)\n",
    "        print(f'printing u{u}')\n",
    "\n",
    "        # For simplicity assume y as known here. For details please refer to the paper.\n",
    "        y = np.array([-1 if n == 0 else n for n in y], dtype=np.float32).reshape(-1, 1)\n",
    "        y = y.mean() if y.mean() != 0 else 0.1\n",
    "        \n",
    "        # print(f'pred_: {u/y:.1e}, udldu: {udldu:.1e}, udldu_:{-u/(1+np.exp(u)):.1e}')\n",
    "        k = -y/(1+np.exp(u))\n",
    "        k = k.reshape(1, -1).astype(np.float32)\n",
    "\n",
    "    else:\n",
    "        # derive activation function\n",
    "        if isinstance(modules[i].act, nn.LeakyReLU):\n",
    "            da = derive_leakyrelu(x_, slope=modules[i].act.negative_slope)\n",
    "        elif isinstance(modules[i].act, nn.Identity):\n",
    "            da = derive_identity(x_)\n",
    "        else:\n",
    "            ValueError(f'Please implement the derivative function of {modules[i].act}')\n",
    "\n",
    "        # back out neuron output\n",
    "        if isinstance(modules[i].act, nn.LeakyReLU):\n",
    "            out = inverse_leakyrelu(x_, slope=modules[i].act.negative_slope)\n",
    "        elif isinstance(modules[i].act, nn.Identity):\n",
    "            out = inverse_identity(x_)\n",
    "        else:\n",
    "            ValueError(f'Please implement the inverse function of {modules[i].act}')\n",
    "        if hasattr(modules[i-1].layer, 'padding'):\n",
    "            padding = modules[i-1].layer.padding[0]\n",
    "        else:\n",
    "            padding = 0\n",
    "\n",
    "        # For a mini-batch setting, reconstruct the combination\n",
    "        in_shape = np.array(x_shape[i-1])\n",
    "        in_shape[0] = 1\n",
    "        # peel off padded entries\n",
    "        x_mask = peeling(in_shape=in_shape, padding=padding)\n",
    "        k = np.multiply(np.matmul(last_weight.transpose(), k)[x_mask], da.transpose())\n",
    "\n",
    "    if isinstance(modules[i].layer, nn.Conv2d):\n",
    "        x_, last_weight = r_gap(out=out, k=k, x_shape=x_shape[i], module=modules[i], g=g, weight=w)\n",
    "    else:\n",
    "        # In consideration of computational efficiency, for FCN only takes gradient constraints into account.\n",
    "        x_, last_weight = fcn_reconstruction(k=k, gradient=g), w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "943f225a-43b7-4a28-b2b1-97d6f10f26c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[122.79957  124.69604  116.46572  117.82588  121.91415  143.97783\n",
      " 116.625984 115.23172  127.89496  101.243225 110.73123   97.896164\n",
      " 106.43551  124.52461  108.81148  119.211815 113.19368   74.29433\n",
      " 121.63076  140.24008  124.68772  108.51068  109.73837  104.542656\n",
      " 111.83563  102.150604 119.3809   122.17023  112.373566 113.37137\n",
      " 118.82995  115.59979  122.63084  104.519615  82.118095 114.10348\n",
      " 116.451866 103.22777  115.99369  116.560585 119.66124  110.96065\n",
      " 111.21297  127.58522  128.6648   113.932365  85.762566 112.033394\n",
      "  80.89454  116.66466  103.426    129.9788   104.402054 101.11888\n",
      "  85.10126  107.71354  116.76297  100.45482  139.55394  106.17704\n",
      " 124.23931   96.94746   99.37204  134.34943 ]\n"
     ]
    }
   ],
   "source": [
    "print(x_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "33410f0f-20d4-408b-9396-2a69a8a75f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.6594, 1.6851, 1.5739, 1.5922, 1.6475, 1.9456, 1.5760, 1.5572, 1.7283,\n",
      "         1.3682, 1.4964, 1.3229, 1.4383, 1.6828, 1.4704, 1.6110, 1.5296, 1.0040,\n",
      "         1.6437, 1.8951, 1.6850, 1.4664, 1.4830, 1.4127, 1.5113, 1.3804, 1.6133,\n",
      "         1.6509, 1.5186, 1.5320, 1.6058, 1.5622, 1.6572, 1.4124, 1.1097, 1.5419,\n",
      "         1.5737, 1.3950, 1.5675, 1.5751, 1.6170, 1.4995, 1.5029, 1.7241, 1.7387,\n",
      "         1.5396, 1.1590, 1.5140, 1.0932, 1.5765, 1.3976, 1.7565, 1.4108, 1.3665,\n",
      "         1.1500, 1.4556, 1.5779, 1.3575, 1.8859, 1.4348, 1.6789, 1.3101, 1.3429,\n",
      "         1.8155]])\n"
     ]
    }
   ],
   "source": [
    "print(ground_truth_feature_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d62ceb-bef5-4aa9-8cb8-fb1d1080933f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
